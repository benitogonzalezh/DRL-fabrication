# -*- coding: utf-8 -*-
"""Array Duplicator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16DfkuUfH6zlJXMpRjkCE6kf9sZiHJiF9
"""

!/usr/local/cuda/bin/nvcc --version

!nvidia-smi

# Commented out IPython magic to ensure Python compatibility.
# Stable Baselines only supports tensorflow 1.x for now
# %tensorflow_version 1.x
!pip -q install stable-baselines[mpi]==2.10.0

cd drive/MyDrive/PaintGame_Gym/

max_step = 40

import gym
import numpy as np

class BasicEnv(gym.Env):

  metadata = {'render.modes': ['console']}

  def initstate(self):
    self.steps = 0
    self.target = [0,1,0,0]
    self.field = [0] * len(self.target)
    self.gridsize = len(self.field)
    self.agentpos = self.gridsize - 1
    self.inside = False

  def __init__(self):
    super(BasicEnv, self).__init__();

    self.initstate()
    #Left, Rigth, In, Out
    self.action_space = gym.spaces.Discrete(4)

    min = np.concatenate((np.array([0]*self.gridsize*2), np.array([0])))
    max = np.concatenate((np.array([1]*self.gridsize*2), np.array([self.gridsize-1])))
    self.observation_space = gym.spaces.Box(low=min, high=max, dtype=np.float32)

  def step(self, action):
    reward = 0.0
    if action == 0:
      self.agentpos -= 1
    elif action == 1:
      self.agentpos += 1
    elif action == 2:
      self.inside = True
    elif action == 3:
      self.inside = False
    else:
      raise ValueError("Accion invalida action={}, no forma parte del espacio de acciones determinado".format(action))

    self.agentpos = np.clip(self.agentpos, 0, self.gridsize - 1)

    if self.inside:
      if self.field[self.agentpos] == 1:
        reward -= 0.05
      else:
        self.field[self.agentpos] = 1

    reward += self.calculatereward(np.array(self.target), np.array(self.field))
    
    info = {}

    if self.target == self.field:
      info = {'message': 'iguales'}
      reward += 10
      print(self.field)
      done = True
    elif self.steps == max_step:
      info = {'message': '100pasos'}
      reward -+ 10
      done = True
    else:
      done = False


    observations = self.getobservations()
    state = observations
    self.steps += 1
    
    reward -= self.steps*0.01

    return state, reward, done, info

  def reset(self):
    self.initstate()
    return self.getobservations()

  def render(self, mode='console'):
    print(self.field)

  def calculatereward(self, target, field):
    if np.sum(field) == 0:
      cos_sim = 0
    else:
      A = np.insert(target,0,1)
      B = np.insert(field,0,1)
      cos_sim=np.dot(A,B)/(np.linalg.norm(A)*np.linalg.norm(B))
    return cos_sim
  
  def getobservations(self):
    return np.concatenate((np.array(self.target), np.array(self.field),np.array([self.agentpos])))

from stable_baselines.common.env_checker import check_env

env = BasicEnv()
check_env(env, warn=True)

env = BasicEnv()

obs = env.reset()
#env.render()

GO_LEFT = 0

n_steps = 2
for step in range(n_steps):
  print("Step {}".format(step + 1))
  obs, reward, done, info = env.step(GO_LEFT)
  env.render()
  if done:
    print("Goal reached!", "reward=", reward)
    break

from stable_baselines import DQN, PPO2, A2C, ACKTR
from stable_baselines.common.cmd_util import make_vec_env

env = BasicEnv()

model = PPO2('MlpPolicy', env, verbose=0, tensorboard_log='./tensorboard/').learn(50000)

# Test the trained agent
obs = env.reset()
n_steps = max_step
for step in range(n_steps):
  action, _ = model.predict(obs)
  print("Step {}".format(step + 1))
  print("Action: ", action)
  obs, reward, done, info = env.step(action)
  env.render()
  print('reward=', reward, 'done=', done)
  if done:
    # Note that the VecEnv resets automatically
    # when a done signal is encountered
    print("Goal reached!", "reward=", reward, info)
    break



# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir './tensorboard'